# Step 1: Trigger Glue Crawler to scan HDFS and register schema
def trigger_glue_crawler(crawler_name):
    glue.start_crawler(Name=crawler_name)
    while glue.get_crawler(Name=crawler_name)["Crawler"]["State"] != "READY":
        time.sleep(10)  # Wait until crawler completes


# Step 2: Retrieve schema from Glue Data Catalog
def get_schema_from_catalog(database_name, table_name):
    table = glue.get_table(DatabaseName=database_name, Name=table_name)
    return table["Table"]["StorageDescriptor"]["Columns"]


# Step 3: Read posting list data from HDFS
def read_posting_lists(hdfs_path, schema):
    raw_data = hdfs.read(hdfs_path)  # Assume HDFS client is available
    parsed_data = parse_with_schema(raw_data, schema)
    return parsed_data  # List of dicts: {term, doc_id, position, timestamp}


# Step 4: Index and organize posting lists
def build_posting_index(parsed_data):
    index = {}
    for entry in parsed_data:
        term = entry["term"]
        doc_id = entry["doc_id"]
        position = entry["position"]
        index.setdefault(term, []).append(
            {"doc_id": doc_id, "position": position, "timestamp": entry["timestamp"]}
        )
    return index  # Dict: term â†’ list of postings


# Step 5: Store posting lists in S3
def store_postings_to_s3(index, bucket_name, prefix):
    for term, postings in index.items():
        key = f"{prefix}/{term}.json"
        s3.put_object(Bucket=bucket_name, Key=key, Body=json.dumps(postings))


# Step 6: Store metadata in DynamoDB
def store_metadata_to_dynamodb(index, table_name):
    for term, postings in index.items():
        metadata = {
            "term": {"S": term},
            "doc_count": {"N": str(len(set(p["doc_id"] for p in postings)))},
            "posting_count": {"N": str(len(postings))},
            "last_updated": {"S": datetime.utcnow().isoformat()},
        }
        dynamodb.put_item(TableName=table_name, Item=metadata)


# Main orchestration function
def depth_microservice_ingestion_pipeline():
    # Trigger Glue Crawler
    trigger_glue_crawler("PostingListCrawler")

    # Get schema from Glue Data Catalog
    schema = get_schema_from_catalog("PostingListDB", "PostingListTable")

    # Read and parse posting lists from HDFS
    parsed_data = read_posting_lists("/hdfs/posting_lists/2025-08-09", schema)

    # Build term-based posting index
    index = build_posting_index(parsed_data)

    # Store posting lists in S3
    store_postings_to_s3(index, "log-search-postings", "postings/2025-08-09")

    # Store metadata in DynamoDB
    store_metadata_to_dynamodb(index, "PostingListMetadata")
